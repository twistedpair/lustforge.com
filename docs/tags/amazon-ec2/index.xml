<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amazon Ec2 on lust.dev</title>
    <link>https://lust.dev/tags/amazon-ec2/</link>
    <description>Recent content in Amazon Ec2 on lust.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Joseph Lust</copyright>
    <lastBuildDate>Sat, 16 Nov 2013 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lust.dev/tags/amazon-ec2/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>AWS EC2 Spot Market Volatility</title>
      <link>https://lust.dev/2013/11/16/aws-ec2-spot-market-volatility/</link>
      <pubDate>Sat, 16 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>https://lust.dev/2013/11/16/aws-ec2-spot-market-volatility/</guid>
      <description>

&lt;p&gt;The spot instance market does not often make sense. You first learn of it and think &amp;#8220;Wow! Why did I waste that money on a reserved instance when I can get the same price on demand?!?!?!&amp;#8221; Then you see pricing curves like this:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/aws_price_curve.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;$6/hr for an instance!? &lt;strong&gt;5000% markup!&lt;/strong&gt; That is stultifying. This &lt;code&gt;m1.medium&lt;/code&gt; is a perfectly fungible resource. You can:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Pay on-demand rate of &lt;strong&gt;$0.12/hr&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Reserve it for 3 years and pay a combined rate (amortizing down payment) of &lt;strong&gt;$0.042/hr&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Rent it at the median Spot Market rate of &lt;strong&gt;$0.013/hr&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$6/hr is 30% more than that most expensive instance AWS sells (&lt;code&gt;hs1.8xlarge&lt;/code&gt;, $4.60/hr), and this instance is puny by comparison.&lt;/p&gt;

&lt;p&gt;So, what gives? Are there really people that are so gormless they shovel money to Amazon? Impossible. Perhaps one foolhardy API consumer put in too high a bid, but we&amp;#8217;re talking aÂ &lt;strong&gt;$6 market clearing price&lt;/strong&gt;. That means theÂ &lt;strong&gt;entire market&lt;/strong&gt; is going for $6. You can&amp;#8217;t get any instance for less. Also note that the price is always going to $6 in a step function, while in a real market, with sentient traders, we&amp;#8217;d expect to just barely outbid our competitors as anything else is waste. You don&amp;#8217;t see people winning an auction by 5000% do you? You pay $0.01+ and call it a victory.&lt;/p&gt;

&lt;p&gt;The only explanation is that during these hours Amazon (or someone with money to burn) is shutting down the market. They don&amp;#8217;t want the spot pool available, perhaps for maintenance reasons or some internal load shaping. Regardless, it renders much of the market useless and highly inefficient.&lt;/p&gt;

&lt;h3 id=&#34;other-observations-8211-highly-non-correlated-market-demand:69d49a87d884be20f9eec90424a302e9&#34;&gt;Other Observations &amp;#8211;Â Highly Non-Correlated Market Demand&lt;/h3&gt;

&lt;p&gt;Markets are often correlated. If the price for beer in a 1 liter container is up, we&amp;#8217;d expect to see a similar rise in demand for beer in the 2 liter container. However, demand for compute is oddly inconsistent across regions, zones, and instance type.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Highly different demand between AZ&amp;rsquo;s (i.e. &lt;code&gt;us-east-1d&lt;/code&gt; is low, &lt;code&gt;us-east-1c&lt;/code&gt; is high)&lt;/li&gt;
&lt;li&gt;Highly different demand between instance types (&lt;code&gt;c3.large&lt;/code&gt; low, &lt;code&gt;m1.medium&lt;/code&gt; high)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If the price for work in one AZ is much higher than another, near by AZ, we&amp;#8217;d expect to see work migrate there. Sure, some jobs might need very high intra-AZ networking speeds, but most can (and should) be spread across AZ&amp;#8217;s. In the 3 months of data analyzed we see notable spot price disparities between neighboring zones.&lt;/p&gt;

&lt;p&gt;Instance demand is also illogical. For example, why is the &lt;code&gt;c3.large&lt;/code&gt; instance, which is far better than a &lt;code&gt;m1.large&lt;/code&gt; instance, always available for $0.032/hr? Over the last three months, there is not a spike to be seen. This makes the &lt;strong&gt;average clearing price for a &lt;code&gt;c3.large&lt;/code&gt;&lt;/strong&gt; ($0.032/hr) &lt;strong&gt;97% lower&lt;/strong&gt; &lt;strong&gt;than an &lt;em&gt;m1.medium&lt;/em&gt;&lt;/strong&gt; ($1.26). That&amp;#8217;s odd, given this is a compute node, of which you&amp;#8217;d expect to see large numbers being recruited and released for brief computation cluster bursts. For your shillings, you&amp;#8217;d likely be better with a &lt;code&gt;c3.large&lt;/code&gt; than a &lt;code&gt;m1.medium&lt;/code&gt;, but the market data does not indicate consumers realize this as the price forÂ &lt;code&gt;c3.large&lt;/code&gt; instances is a flatline.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/aws_price_curve_2.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h3 id=&#34;conclusions:69d49a87d884be20f9eec90424a302e9&#34;&gt;Conclusions&lt;/h3&gt;

&lt;p&gt;AWS originally marketed the spot market as allowing you to run work at &lt;em&gt;different times&lt;/em&gt; for more optimal pricing.Â But, you should also ask yourself:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Do you need an on-demand instance when a spot will do?&lt;/li&gt;
&lt;li&gt;Do you really need your spot in the same AZ or region?&lt;/li&gt;
&lt;li&gt;Can you run your AMI on a different instance type?&lt;/li&gt;
&lt;li&gt;Will someone manipulating the market price, pushing it beyond reasonable viability?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now if only EC2 would allow bidding across a range of instance types to run your jobs, folks could get more done with less.&lt;/p&gt;

&lt;p&gt;P.S. These are just some observations from poking around the data a couple minutes. If you have more piercing insights, please elaborate below.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LogRotate Apache Logs to Amazon S3</title>
      <link>https://lust.dev/2012/07/15/logrotate-apache-logs-to-amazon-s3/</link>
      <pubDate>Sun, 15 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://lust.dev/2012/07/15/logrotate-apache-logs-to-amazon-s3/</guid>
      <description>


&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/logrotate.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Legacy Log Rotation before gzip &amp;amp; scp&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;I recently moved my site &lt;a href=&#34;www.runpartner.com&#34;&gt;RunPartner&lt;/a&gt; to Amazon Web Services (AWS) from DreamHost because for just a few more peanuts a month I got a ton of enterprise grade services, and the server does not crash randomly any more. ðŸ˜Š&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m loving AWS, but one thing I wanted to do was consolidate all logs to S3. Let&amp;#8217;s say your site gets SlashDotted or Pinned. One of the first failure modes is that your logs swell up, and you&amp;#8217;re out of disk space. Since my EC2 instance has just 8GB, this is possible. But why not use that infinite storage pool in the sky, S3? Perfect.&lt;/p&gt;

&lt;h2 id=&#34;logrotate:d6ae9d679337e62320eea662108a42c7&#34;&gt;LogRotate&lt;/h2&gt;

&lt;p&gt;A long time fixture of the log rotation scene is &lt;a title=&#34;Man page&#34; href=&#34;http://linux.die.net/man/8/logrotate&#34; target=&#34;_blank&#34;&gt;LogRotate&lt;/a&gt;, not to be confused with Apache&amp;#8217;s &lt;a href=&#34;http://httpd.apache.org/docs/2.0/programs/rotatelogs.html&#34; target=&#34;_blank&#34;&gt;RotateLogs&lt;/a&gt;. The following script works well for me (two yearsÂ and counting).&lt;/p&gt;

&lt;p&gt;The script assumes you&amp;#8217;ve installed the amazing command line package, &lt;a href=&#34;http://s3tools.org/s3cmd&#34;&gt;s3cmd&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# rotate the logs!
# common settings
compress
compresscmd /bin/gzip
compressoptions -9
compressext .gz

dateext
dateformat -%Y-%m-%d-%s

rotate 3
nomail
missingok
daily
size 5k
create 640 username username

/var/logs/www.runpartner.com/*.log {
sharedscripts
postrotate
sudo /usr/sbin/apache2ctl graceful

/usr/bin/s3cmd sync /var/logs/www.runpartner.com/*.gz s3://bucket-logs/www.runpartner.com/
endscript
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-it-works:d6ae9d679337e62320eea662108a42c7&#34;&gt;How It Works&lt;/h2&gt;

&lt;p&gt;It took me a few hours to get everything tweaked just right, so I&amp;#8217;ll break down the commands for your edification and so that you can customize the script for yourself.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;compress
compresscmd /bin/gzip
compressoptions -9
compressext .gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;compress&lt;/code&gt; - enables compression&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compresscmd&lt;/code&gt; - determines the path to the utility used to compress&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compressoptions&lt;/code&gt; - command line switches passed to the compression utility&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compressext&lt;/code&gt; - this suffix will be used to determine if files have been compressed&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dateext
dateformat -%Y-%m-%d-%s
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dateext&lt;/code&gt; - enables adding dates to the log file names&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dateformat&lt;/code&gt; - &lt;code&gt;%Y-%m-%d-%s&lt;/code&gt; provides the format for the log file name dates&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rotate 3
nomail
missingok
daily
size 5k
create 640 username username
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rotate 3&lt;/code&gt; - how many logs to keep locally before deleting. The more, the more space used&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nomail&lt;/code&gt; - don&amp;#8217;t try to mail the logs to any body&lt;/li&gt;
&lt;li&gt;&lt;code&gt;missingok&lt;/code&gt; - tells script not to freak out that there are no files on first run&lt;/li&gt;
&lt;li&gt;&lt;code&gt;daily&lt;/code&gt; - rollover logs on a daily basis (&lt;strong&gt;still must call from Cron though&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size 5K&lt;/code&gt; - set minimum size of log rollover. If file is smaller than this, it will not be rolled.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;create 640 username username&lt;/code&gt; - add any permissions the files should be given on creation. I needed these, or the compression utility did not have the right to compress them.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/var/logs/www.runpartner.com/*.log {
sharedscripts
postrotate
sudo /usr/sbin/apache2ctl graceful

/usr/bin/s3cmd sync /var/logs/www.runpartner.com/*.gz s3://bucket-logs/www.runpartner.com/
endscript
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/logs/www.runpartner.com/*.log&lt;/code&gt; - file selector, can contain wildcards, or can be explicit&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sharedscripts&lt;/code&gt; - only run the code between &lt;em&gt;postrotate&lt;/em&gt; &amp;amp; &lt;em&gt;endscript&lt;/em&gt; &lt;strong&gt;once&lt;/strong&gt;, even if multiple files rotated&lt;/li&gt;
&lt;li&gt;&lt;code&gt;postrotate...endscript&lt;/code&gt; -code to send logs to S3

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo /usr/sbin/apache2ctl graceful&lt;/code&gt; gracefully resets the logs on the apache server, otherwise logging stops because LogRotate removed the log file! Some people just kill the pid, but this is much cleaner.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/usr/bin/s3cmd sync /var/logs/www.runpartner.com/*.gz s3://bucket-logs/www.runpartner.com/&lt;/code&gt; use s3cmd to sync your logs (not many of them, so goes fast) to your log bucket on S3.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cron-it:d6ae9d679337e62320eea662108a42c7&#34;&gt;Cron it&lt;/h2&gt;

&lt;p&gt;Now just don&amp;#8217;t forget the last step!!! You need to add your LogRotate command to cron, so it can run each day. If you have a lot of traffic, you might want to run it on the hour, with your &lt;em&gt;size&lt;/em&gt; attribute set so that large logs get moved to S3 quickly, freeing up space on your instance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Backup activities
0 0 * * * /usr/sbin/logrotate --state /home/username/scripts/log_rotate.state /home/username/scripts/log_rotate.config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the below I run the rollover at midnight. This way all entries in the log for a given date are really for that date. Also,Â &lt;strong&gt;don&amp;#8217;t forget the state file&lt;/strong&gt;. This is how LogRotate knows what it did last time, so it can decide what to do this time. Finally, notice that cron needs the full path to everything, including theÂ executables and the config/state files.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>