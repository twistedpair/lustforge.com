<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amazon S3 on lust.dev</title>
    <link>https://lust.dev/tags/amazon-s3/</link>
    <description>Recent content in Amazon S3 on lust.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Joseph Lust</copyright>
    <lastBuildDate>Sat, 28 Feb 2015 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lust.dev/tags/amazon-s3/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Don’t use Git to Deploy Code</title>
      <link>https://lust.dev/2015/02/28/dont-use-git-to-deploy-code/</link>
      <pubDate>Sat, 28 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://lust.dev/2015/02/28/dont-use-git-to-deploy-code/</guid>
      <description>

&lt;!--Not again GitHub! Save us Muscular Failure Unicorn!--&gt;

&lt;p&gt;Just don&amp;#8217;t. If you can&amp;#8217;t reason why, please stop developing code critical to your business.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/github_down.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;h3 id=&#34;when-it-works-it-works:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;When it works, it works&lt;/h3&gt;

&lt;p&gt;Git is great. &lt;em&gt;Push&lt;/em&gt; here, &lt;em&gt;pull&lt;/em&gt; there. It works so well that you might be fatuously convinced it&amp;#8217;s the perfect tool to deploy production code. Even worse, it might appear to work well at this task, further reinforcing your choice  However, the Achilles Heel of any DVCS is your origin provider. Let&amp;#8217;s say that &lt;a href=&#34;https://twitter.com/bitbucket/status/460418803805855745&#34;&gt;BitBucket&lt;/a&gt; has &lt;a href=&#34;https://twitter.com/search?q=%40BitBucket%20down&amp;amp;src=typd&#34;&gt;borked&lt;/a&gt; their database for the 4th time this month or GitHub is suffering yet another DDNS attack. Then we see &lt;a href=&#34;http://jorgennilsson.com/article/there-is-always-a-drawback-bitbucket-major-outage&#34;&gt;posts&lt;/a&gt; &lt;a href=&#34;https://bitbucket.org/alixandru/bitbucket-sync/issue/19/post-hook-deployment-stopped-working&#34;&gt;opining&lt;/a&gt; about failed Git based app deployments.&lt;/p&gt;

&lt;h3 id=&#34;when-shit-goes-wrong-things-get-complicated:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;When shit goes wrong, things get complicated&lt;/h3&gt;

&lt;p&gt;Now shit&amp;#8217;s gone wrong. No worries, there must be a more complicated way to solve what appeared to be a simple workflow. We&amp;#8217;ve got all these Unix cli tools and can bodger something together. I think I can just scp the files over. Wait, better rsync them, I&amp;#8217;m not sure exactly which ones changed. Arr&amp;#8230; so many flags, do I want to compare file checksums or timestamps? Maybe I&amp;#8217;ll tarball up everything and push it over to the servers. What was the command string again &lt;a href=&#34;https://xkcd.com/1168/&#34;&gt;to untar&lt;/a&gt; and ungzip? Crap, I included my file permissions and they don&amp;#8217;t work on the server. Huh, how was I supposed to know the code stored running PID&amp;#8217;s in various files sprinkled throughout the source? WTF, someone tweaked some of those settings files server side and I just overwrote them. Fuck&amp;#8230; I made backup of that server directory before I started, right? Alright, Hail Mary time, I&amp;#8217;ll just export my remote repo and import it as a different origin on the server. How the hell do I do that?&lt;/p&gt;

&lt;h3 id=&#34;shit-goes-wrong-at-the-wrong-time:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;Shit goes wrong at the wrong time&lt;/h3&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/bitbucket_down.png&#34; /&gt;
    
    
&lt;/figure&gt;


&lt;p&gt;No knowledge of HTTP codes necessary at BitBucket (500 is bad).&lt;/p&gt;

&lt;p&gt;The above might be a fun exercise on the QA server when it&amp;#8217;s 3pm and everyone&amp;#8217;s in the office on a slow Tuesday, but that&amp;#8217;s not how these things unfold. Nope. What will really happen is there is a hotfix that needs to go out and got assigned to the intern, because he needs the experience, you know. And because he&amp;#8217;s the only guy on call during Thanksgiving since everyone else is away on vacation. But now he&amp;#8217;s riding the wheels off this Rube Goldberg machine, getting both hands stuck in the tar pit and only working himself deeper as he borks the entire production setup and your site is down for the count at 2am on Black Friday.&lt;/p&gt;

&lt;h3 id=&#34;special-snowflake-servers:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;Special snowflake servers&lt;/h3&gt;

&lt;p&gt;Git checkouts to update code encourage &lt;a href=&#34;http://martinfowler.com/bliki/SnowflakeServer.html&#34;&gt;Special Snowflake servers&lt;/a&gt;. Each server is a unique, artisan crafted piece of Unix art. Like literal snowflakes, no two are the same. No one really understands how it all works and the little documentation that exists was last updated in the Bush administration.  Running &lt;code&gt;git status&lt;/code&gt; shows lots of little file changes to get things just right on each machine, some versioned, some not, so no one has had the balls to &lt;code&gt;git reset --hard&lt;/code&gt; for years now.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/fail_bitbucket.png&#34; /&gt;
    
    
&lt;/figure&gt;

Satanic BitBucket Logo of Doom&lt;/p&gt;

&lt;h3 id=&#34;the-better-deterministic-way:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;The better, deterministic way&lt;/h3&gt;

&lt;p&gt;Deploy your code as a self contained distributable. In Java we&amp;#8217;ve got &lt;a href=&#34;http://en.wikipedia.org/wiki/WAR_%28file_format%29&#34;&gt;War&lt;/a&gt; and &lt;a href=&#34;http://en.wikipedia.org/wiki/EAR_%28file_format%29&#34;&gt;Ear&lt;/a&gt; files. In Play Framework we&amp;#8217;ve got binary distributables you unzip and run. ASP.Net can be &lt;a href=&#34;https://msdn.microsoft.com/en-us/library/dd465323%28v=vs.110%29.aspx&#34; target=&#34;_blank&#34;&gt;packaged&lt;/a&gt;, just like &lt;a href=&#34;https://www.ruby-toolbox.com/categories/packaging_to_executables&#34; target=&#34;_blank&#34;&gt;Ruby&lt;/a&gt; and many others. They&amp;#8217;re like &lt;a href=&#34;http://en.wikipedia.org/wiki/Meal,_Ready-to-Eat&#34; target=&#34;_blank&#34;&gt;MRE&lt;/a&gt;&amp;#8216;s, but you just unzip them. No need to add water. You don&amp;#8217;t care what version of Scala is running on the server host, whether the proper DLL is loaded, or if you&amp;#8217;re on the proper Ruby release. It Just Works™. When shit&amp;#8217;s broken and your customers are screaming on Twitter, you want your code to Just Work.&lt;/p&gt;

&lt;h3 id=&#34;distributing-the-distributables:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;Distributing the distributables&lt;/h3&gt;

&lt;p&gt;&amp;#8220;The distributable is huge!&amp;#8221; you warn. Sure, 78MB won&amp;#8217;t fit on a floppy, but we&amp;#8217;ve got 10G server interconnects. I think we&amp;#8217;ll be OK. &amp;#8220;But where will we server those from,&amp;#8221; you say, still unconvinced. How about &lt;a href=&#34;http://aws.amazon.com/s3/&#34; target=&#34;_blank&#34;&gt;AWS S3&lt;/a&gt;, with 11 nines durability (99.999999999). Or, you can setup your own &lt;a href=&#34;http://docs.openstack.org/developer/swift/&#34; target=&#34;_blank&#34;&gt;Open Stack Swift&lt;/a&gt; object store if you&amp;#8217;d prefer.&lt;/p&gt;

&lt;p&gt;The process is simple:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Built and unit/integration test a commit on CI&lt;/li&gt;
&lt;li&gt;Push passing build distributable to S3&lt;/li&gt;
&lt;li&gt;Your deploy script on server downloads from S3 and restarts the app&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;If S3 is down (better take some real MRE&amp;#8217;s into the basement, the end is near), you either:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Download the distributable artifact from CI and scp it to the server&lt;/li&gt;
&lt;li&gt;If CI and S3 are down, build locally and scp it to the server&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The point is to have a canonical way to turn an explicit state of source (i.e. checkout hash) into an binary that will consistently run as desired where you deploy it. No chasing thousands of source files. No trying to compile all the source on your workstation, and on your CI, and on your front end servers. Fight entropy. Choose determinism.&lt;/p&gt;

&lt;h3 id=&#34;other-reasons:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;Other Reasons&lt;/h3&gt;

&lt;h4 id=&#34;file-contention:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;File contention&lt;/h4&gt;

&lt;p&gt;Do you work in one of those _scripting_ languages? Say PHP, Ruby, or Python. Ever had your SCM fail to update files because of open file pointers to running or zombie threads? Prepare yourself for some possible non-deterministic behavior when you deploy these apps via Git. It&amp;#8217;ll be best to add some &lt;em&gt;pgrep&lt;/em&gt; steps to run around and kill off possibly offensive threads, but then you&amp;#8217;ve got to be asking yourself, &amp;#8220;what life choices did I make that led me to run around killing myriad threads on each deploy?&amp;#8221;&lt;/p&gt;

&lt;h4 id=&#34;scm-8217-s-worse-than-git:9caea413e602ba8b2ea3301e2cd987db&#34;&gt;SCM&amp;#8217;s worse than git&lt;/h4&gt;

&lt;p&gt;Git works pretty well, but what if you&amp;#8217;re deploying with another SCM like &lt;strong&gt;SVN&lt;/strong&gt;. God help you, friend. The change databases that back your local SVN checkout can get corrupted in &lt;a href=&#34;https://stackoverflow.com/questions/335987/how-can-a-svn-repository-become-corrupt&#34;&gt;wondrous ways&lt;/a&gt;. The net result can be that SVN says you&amp;#8217;re on revision X, and &lt;code&gt;svn status&lt;/code&gt; shows no files are changed locally. When you call &lt;code&gt;svn update&lt;/code&gt; or checkout the target revision, you&amp;#8217;re told you&amp;#8217;re already up to date&amp;#8230; but &lt;strong&gt;you&amp;#8217;re not&lt;/strong&gt;.&lt;strong&gt; &lt;/strong&gt;This is true FML territory. If your SCM cannot reliably track changes, it should be cast to special circle in hell. Sadly, I&amp;#8217;ve personally seen this happen three times in a single year. God help you, friend.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Subdomain SSL Certificate Verification on Route 53</title>
      <link>https://lust.dev/2014/03/16/subdomain-ssl-certificate-verification-on-route-53/</link>
      <pubDate>Sun, 16 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://lust.dev/2014/03/16/subdomain-ssl-certificate-verification-on-route-53/</guid>
      <description>

&lt;h3 id=&#34;sadly-a-godaddy-user:b7a430c4afc0956461a988f48a78451d&#34;&gt;Sadly a GoDaddy User&lt;/h3&gt;

&lt;p&gt;I use GoDaddy for a few domains and some SSL. Every time I need to set something up, it is like pulling teeth. First you&amp;#8217;ve got to get through four layers of sales pitches, cross sells, and upsells. Then you finally get to the admin UI&amp;#8217;s. Not the UI, the UI&amp;#8217;s. Each one of them appears to have been designed by team with a different design philosophy. To boot the doc&amp;#8217;s appear to have been written by someone that never actual used the UI they&amp;#8217;re prescribing the use of.&lt;/p&gt;

&lt;h3 id=&#34;ssl-domain-verification:b7a430c4afc0956461a988f48a78451d&#34;&gt;SSL Domain Verification&lt;/h3&gt;

&lt;p&gt;Here&amp;#8217;s the problem. You&amp;#8217;ve got 2 domains to issue SSL certificates on. Let&amp;#8217;s say&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;foo.com&lt;/li&gt;
&lt;li&gt;staging.foo.com&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;According to the doc&amp;#8217;s you need to put a TXT record on &amp;#8220;dzc.&amp;#8221; + your domain. OK, so that would be&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dzc.foo.com TXT=&amp;#8221;fooVal1&amp;#8243;&lt;/li&gt;
&lt;li&gt;dzc.staging.foo.com TXT=&amp;#8221;fooVal2&amp;#8243;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Great, looks good. What? Why is it that only one SSL domain is validated and the cert issued? We&amp;#8217;ll, it&amp;#8217;s your fault for &lt;a href=&#34;http://support.godaddy.com/help/article/4678/creating-a-txt-record-for-ssl-validation?locale=en&#34;&gt;following the directions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The rub is that &lt;strong&gt;all subdomain TXT values must be placed on the root domain&lt;/strong&gt;. Awesome. Does everyone in control of a subdomain have control of the root? Certainly not. Hopefully you learned this by trial and error.&lt;/p&gt;

&lt;h3 id=&#34;hope-it-helps:b7a430c4afc0956461a988f48a78451d&#34;&gt;Hope it Helps&lt;/h3&gt;

&lt;p&gt;I&amp;#8217;ve jotted down these little ditties as time and time again, I have to experiment with GoDaddy settings as following them to the letter rarely works. GoDaddy, please read your own docs!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LogRotate Apache Logs to Amazon S3</title>
      <link>https://lust.dev/2012/07/15/logrotate-apache-logs-to-amazon-s3/</link>
      <pubDate>Sun, 15 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://lust.dev/2012/07/15/logrotate-apache-logs-to-amazon-s3/</guid>
      <description>


&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/logrotate.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Legacy Log Rotation before gzip &amp;amp; scp&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;I recently moved my site &lt;a href=&#34;www.runpartner.com&#34;&gt;RunPartner&lt;/a&gt; to Amazon Web Services (AWS) from DreamHost because for just a few more peanuts a month I got a ton of enterprise grade services, and the server does not crash randomly any more. 😊&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m loving AWS, but one thing I wanted to do was consolidate all logs to S3. Let&amp;#8217;s say your site gets SlashDotted or Pinned. One of the first failure modes is that your logs swell up, and you&amp;#8217;re out of disk space. Since my EC2 instance has just 8GB, this is possible. But why not use that infinite storage pool in the sky, S3? Perfect.&lt;/p&gt;

&lt;h2 id=&#34;logrotate:d6ae9d679337e62320eea662108a42c7&#34;&gt;LogRotate&lt;/h2&gt;

&lt;p&gt;A long time fixture of the log rotation scene is &lt;a title=&#34;Man page&#34; href=&#34;http://linux.die.net/man/8/logrotate&#34; target=&#34;_blank&#34;&gt;LogRotate&lt;/a&gt;, not to be confused with Apache&amp;#8217;s &lt;a href=&#34;http://httpd.apache.org/docs/2.0/programs/rotatelogs.html&#34; target=&#34;_blank&#34;&gt;RotateLogs&lt;/a&gt;. The following script works well for me (two years and counting).&lt;/p&gt;

&lt;p&gt;The script assumes you&amp;#8217;ve installed the amazing command line package, &lt;a href=&#34;http://s3tools.org/s3cmd&#34;&gt;s3cmd&lt;/a&gt;.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# rotate the logs!
# common settings
compress
compresscmd /bin/gzip
compressoptions -9
compressext .gz

dateext
dateformat -%Y-%m-%d-%s

rotate 3
nomail
missingok
daily
size 5k
create 640 username username

/var/logs/www.runpartner.com/*.log {
sharedscripts
postrotate
sudo /usr/sbin/apache2ctl graceful

/usr/bin/s3cmd sync /var/logs/www.runpartner.com/*.gz s3://bucket-logs/www.runpartner.com/
endscript
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;how-it-works:d6ae9d679337e62320eea662108a42c7&#34;&gt;How It Works&lt;/h2&gt;

&lt;p&gt;It took me a few hours to get everything tweaked just right, so I&amp;#8217;ll break down the commands for your edification and so that you can customize the script for yourself.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;compress
compresscmd /bin/gzip
compressoptions -9
compressext .gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;compress&lt;/code&gt; - enables compression&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compresscmd&lt;/code&gt; - determines the path to the utility used to compress&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compressoptions&lt;/code&gt; - command line switches passed to the compression utility&lt;/li&gt;
&lt;li&gt;&lt;code&gt;compressext&lt;/code&gt; - this suffix will be used to determine if files have been compressed&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;dateext
dateformat -%Y-%m-%d-%s
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dateext&lt;/code&gt; - enables adding dates to the log file names&lt;/li&gt;
&lt;li&gt;&lt;code&gt;dateformat&lt;/code&gt; - &lt;code&gt;%Y-%m-%d-%s&lt;/code&gt; provides the format for the log file name dates&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;rotate 3
nomail
missingok
daily
size 5k
create 640 username username
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;rotate 3&lt;/code&gt; - how many logs to keep locally before deleting. The more, the more space used&lt;/li&gt;
&lt;li&gt;&lt;code&gt;nomail&lt;/code&gt; - don&amp;#8217;t try to mail the logs to any body&lt;/li&gt;
&lt;li&gt;&lt;code&gt;missingok&lt;/code&gt; - tells script not to freak out that there are no files on first run&lt;/li&gt;
&lt;li&gt;&lt;code&gt;daily&lt;/code&gt; - rollover logs on a daily basis (&lt;strong&gt;still must call from Cron though&lt;/strong&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;size 5K&lt;/code&gt; - set minimum size of log rollover. If file is smaller than this, it will not be rolled.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;create 640 username username&lt;/code&gt; - add any permissions the files should be given on creation. I needed these, or the compression utility did not have the right to compress them.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;/var/logs/www.runpartner.com/*.log {
sharedscripts
postrotate
sudo /usr/sbin/apache2ctl graceful

/usr/bin/s3cmd sync /var/logs/www.runpartner.com/*.gz s3://bucket-logs/www.runpartner.com/
endscript
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/var/logs/www.runpartner.com/*.log&lt;/code&gt; - file selector, can contain wildcards, or can be explicit&lt;/li&gt;
&lt;li&gt;&lt;code&gt;sharedscripts&lt;/code&gt; - only run the code between &lt;em&gt;postrotate&lt;/em&gt; &amp;amp; &lt;em&gt;endscript&lt;/em&gt; &lt;strong&gt;once&lt;/strong&gt;, even if multiple files rotated&lt;/li&gt;
&lt;li&gt;&lt;code&gt;postrotate...endscript&lt;/code&gt; -code to send logs to S3

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;sudo /usr/sbin/apache2ctl graceful&lt;/code&gt; gracefully resets the logs on the apache server, otherwise logging stops because LogRotate removed the log file! Some people just kill the pid, but this is much cleaner.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/usr/bin/s3cmd sync /var/logs/www.runpartner.com/*.gz s3://bucket-logs/www.runpartner.com/&lt;/code&gt; use s3cmd to sync your logs (not many of them, so goes fast) to your log bucket on S3.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;cron-it:d6ae9d679337e62320eea662108a42c7&#34;&gt;Cron it&lt;/h2&gt;

&lt;p&gt;Now just don&amp;#8217;t forget the last step!!! You need to add your LogRotate command to cron, so it can run each day. If you have a lot of traffic, you might want to run it on the hour, with your &lt;em&gt;size&lt;/em&gt; attribute set so that large logs get moved to S3 quickly, freeing up space on your instance.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Backup activities
0 0 * * * /usr/sbin/logrotate --state /home/username/scripts/log_rotate.state /home/username/scripts/log_rotate.config
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the below I run the rollover at midnight. This way all entries in the log for a given date are really for that date. Also, &lt;strong&gt;don&amp;#8217;t forget the state file&lt;/strong&gt;. This is how LogRotate knows what it did last time, so it can decide what to do this time. Finally, notice that cron needs the full path to everything, including the executables and the config/state files.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>