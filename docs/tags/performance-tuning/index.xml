<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Performance Tuning on lust.dev</title>
    <link>https://lust.dev/tags/performance-tuning/</link>
    <description>Recent content in Performance Tuning on lust.dev</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Joseph Lust</copyright>
    <lastBuildDate>Tue, 04 Nov 2014 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://lust.dev/tags/performance-tuning/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Dynatrace Memory Sensor Anti-Patterns</title>
      <link>https://lust.dev/2014/11/04/dynatrace-memory-sensor-anti-patterns/</link>
      <pubDate>Tue, 04 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>https://lust.dev/2014/11/04/dynatrace-memory-sensor-anti-patterns/</guid>
      <description>

&lt;p&gt;This is a collection of Dynatrace actions I&amp;#8217;ve learned to avoid. I&amp;#8217;ll add more as my Dynatrace journey evolves.&lt;/p&gt;

&lt;h3 id=&#34;judiciously-apply-memory-sensors:4fe8a6a2b35495e8322e41d58f8a305b&#34;&gt;Judiciously Apply Memory Sensors&lt;/h3&gt;

&lt;p&gt;If one Memory Sensor is good, then many must be great? Sadly not. Memory sensors are useful as they allow Selective Memory Snapshots to be taken of a subset of the heap graph without taking an entire heap dump. They are delightfully fast and light, but too many will spoil the party.&lt;/p&gt;

&lt;p&gt;Any scientific instrument will perturb the system under measure during the act of mensuration. With Dynatrace, applying Memory Sensors to core services (singletons) only incurs an ~1ms increase in initialization due to byte code instrumentation and is not a problem. However, if the instrumented object is a core type which is created and garbage collected often, the effects can be shocking.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s look at adding memory sensors to two core types in my application, &lt;strong&gt;LocalDate&lt;/strong&gt; and &lt;strong&gt;Money&lt;/strong&gt;. The application was creating millions of date objects for a certain batch job as well as money objects. I wanted to see how much heap was consumed by them, so I instrumented these objects with Memory Sensors. Suddenly, the application began to crawl.&lt;/p&gt;

&lt;p&gt;Below we see the new application &lt;strong&gt;Hot Spot Methods&lt;/strong&gt;. The init of LocalDate objects takes nearly &lt;strong&gt;4 minutes&lt;/strong&gt;. Similarly the BigDecimals inside the Money objects are consume an inordinate amount of time. This job is a database bound job, but here we see the database calls are only &lt;strong&gt;0.3%&lt;/strong&gt; of the hot spot methods. The Dynatrace instrumentation of these memory sensors is to blame, which might be a surprise as the memory sensors are not even creating Pure Paths and we&amp;#8217;re not taking any Selective Memory Snapshots.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/localDate.png&#34; /&gt;
    
    
&lt;/figure&gt;

Initialization of common objects is consuming 99.7% of job runtime (200 of 3200 job PurePaths shown)&lt;/p&gt;

&lt;p&gt;After the Memory Sensors are removed, we see that query execution dominates the job runtime, as expected. We also see that LocalDate instantiation has dropped from &lt;strong&gt;4 minutes to 20ms&lt;/strong&gt; (too small to appear in Hot Spots report below). The moral of the story? The M&lt;strong&gt;emory Sensors on LocalDate increased it&amp;#8217;s initialization time 12,000 times!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/noLocalDate.png&#34; /&gt;
    
    
&lt;/figure&gt;

12000x faster LocalDate init w/o Memory Sensors (3200 of 3200 job PurePaths shown)&lt;/p&gt;

&lt;p&gt;The CPU and garbage collection times were also  dilated notably by the wanton application of Memory Sensors. Below we can see that GC time is magnified ~14x and CPU consumption more than doubled to 93% from 41%. Note that the Memory Sensor case below is truncated as the sensors were removed via Hot Placement during the job, otherwise it would ostensibly have taken forever to run.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;https://lust.dev/img/gcAndCpuDifference.png&#34; /&gt;
    
    
&lt;/figure&gt;

Memory Sensors also double CPU consumption and GC time&lt;/p&gt;

&lt;p&gt;The moral of the story? &lt;strong&gt;Always instrument the bare minimum necessary items with Dynatrace&lt;/strong&gt;. All measurements have overhead and perturb the code under test. &lt;strong&gt;The greater the instrumentation, the greater the perturbation&lt;/strong&gt;. You want to be confident that the trends you discover in Dynatrace are due to the code under test, rather than an artifact of the instrumentation.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>