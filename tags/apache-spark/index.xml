<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Apache Spark on lust.dev</title><link>https://lust.dev/tags/apache-spark/</link><description>Recent content in Apache Spark on lust.dev</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Joe Lust</copyright><lastBuildDate>Thu, 07 Jan 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://lust.dev/tags/apache-spark/index.xml" rel="self" type="application/rss+xml"/><item><title>Home Cooked Apache Spark</title><link>https://lust.dev/2016/01/07/home-cooked-apache-spark/</link><pubDate>Thu, 07 Jan 2016 00:00:00 +0000</pubDate><guid>https://lust.dev/2016/01/07/home-cooked-apache-spark/</guid><description>We use Apache Spark for various applications at my job, but Spark is still relatively unstable, as evidenced by the project&amp;rsquo;s 11K+ pull requests. To maintain developer velocity, we regularly patch show stopper bugs in the Spark source. The process is simple.
Install JDK 6, which is required for PySpark (or you&amp;rsquo;ll get a lengthy warning). Use the oracle-java8-set-default package to switch between Java 6 and 8, or set JAVA_HOME.</description></item><item><title>Remote Debugging Apache Spark Clusters</title><link>https://lust.dev/2016/01/01/remote-debugging-apache-spark-clusters/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://lust.dev/2016/01/01/remote-debugging-apache-spark-clusters/</guid><description>Debugging Apache Spark can be tricky. Sure, everything works on your --master local[4] cluster, but not when run on a real cluster. In these cases, you need to drop to a debug breakpoint in the running cluster.
Get Debugger Listening Simply update the launch args wherever you start Spark like so. Note: SPARK_JAVA_OPTS won&amp;rsquo;t do the trick.
export SPARK_WORKER_OPTS=&amp;#34;-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n&amp;#34; export SPARK_MASTER_OPTS=&amp;#34;-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n&amp;#34; /opt/spark/sbin/start-master.sh Open an SSH tunnel to your remote cluster machine, mapping localhost:4000 to spark-master.</description></item></channel></rss>