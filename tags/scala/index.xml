<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scala on lust.dev</title><link>https://lust.dev/tags/scala/</link><description>Recent content in Scala on lust.dev</description><generator>Hugo</generator><language>en-us</language><copyright>Joe Lust</copyright><lastBuildDate>Sun, 08 May 2016 00:00:00 +0000</lastBuildDate><atom:link href="https://lust.dev/tags/scala/index.xml" rel="self" type="application/rss+xml"/><item><title>Modulo Operator Performance Impact</title><link>https://lust.dev/2016/05/08/modulo-operator-performance-impact/</link><pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate><guid>https://lust.dev/2016/05/08/modulo-operator-performance-impact/</guid><description>My textbooks used modulo, yet my boss told me not to. Where had I gone wrong?</description></item><item><title>Dangers of the Unit Type Parameter</title><link>https://lust.dev/2016/04/12/dangers-of-unit-type-parameter/</link><pubDate>Tue, 12 Apr 2016 00:00:00 +0000</pubDate><guid>https://lust.dev/2016/04/12/dangers-of-unit-type-parameter/</guid><description>Confusing misuses of Unit and the loss of type checking.</description></item><item><title>Home Cooked Apache Spark</title><link>https://lust.dev/2016/01/07/home-cooked-apache-spark/</link><pubDate>Thu, 07 Jan 2016 00:00:00 +0000</pubDate><guid>https://lust.dev/2016/01/07/home-cooked-apache-spark/</guid><description>&lt;p>We use Apache Spark for various applications at &lt;a href="https://mc10inc.com">my job&lt;/a>, but Spark is still relatively unstable, as evidenced by the project&amp;rsquo;s &lt;a href="https://github.com/apache/spark/pulls">11K+&lt;/a> pull requests. To maintain developer velocity, we regularly patch show stopper bugs in the Spark source. The process is simple.&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Install JDK 6, which is required for PySpark (or you&amp;rsquo;ll get a lengthy warning). Use the &lt;code>oracle-java8-set-default&lt;/code> package to switch between Java 6 and 8, or set &lt;code>JAVA_HOME&lt;/code>.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">sudo apt-get install oracle-java6-installer
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">oracle-java8-set-default &lt;span class="c1"># Go back to Java 8 when you&amp;#39;re done building&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;/li>
&lt;li>
&lt;p>Fork the &lt;a href="https://github.com/apache/spark">Apache Spark repo&lt;/a> so you can submit a Pull Request later&lt;/p></description></item><item><title>Remote Debugging Apache Spark Clusters</title><link>https://lust.dev/2016/01/01/remote-debugging-apache-spark-clusters/</link><pubDate>Fri, 01 Jan 2016 00:00:00 +0000</pubDate><guid>https://lust.dev/2016/01/01/remote-debugging-apache-spark-clusters/</guid><description>&lt;p>Debugging Apache Spark can be tricky. Sure, everything works on your &lt;code>--master local[4]&lt;/code> cluster, but not when run on a real cluster. In these cases, you need to drop to a debug breakpoint in the running cluster.&lt;/p>
&lt;h1 id="get-debugger-listening">Get Debugger Listening&lt;/h1>
&lt;p>Simply update the launch args wherever you start Spark like so. Note: &lt;code>SPARK_JAVA_OPTS&lt;/code> won&amp;rsquo;t do the trick.&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" class="chroma">&lt;code class="language-bash" data-lang="bash">&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">export&lt;/span> &lt;span class="nv">SPARK_WORKER_OPTS&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">export&lt;/span> &lt;span class="nv">SPARK_MASTER_OPTS&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="s2">&amp;#34;-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> /opt/spark/sbin/start-master.sh
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Open an SSH tunnel to your remote cluster machine, mapping &lt;code>localhost:4000&lt;/code> to &lt;code>spark-master.foo.com:5000&lt;/code>, assuming the cluster is at &lt;code>spark-master.foo.com&lt;/code>, listening on port &lt;code>5000&lt;/code>.&lt;/p></description></item></channel></rss>